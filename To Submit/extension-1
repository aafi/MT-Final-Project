#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict, Counter
import operator
from sklearn import linear_model
from nltk.corpus import stopwords
import nltk
from nltk.stem.porter import *

optparser = optparse.OptionParser()
optparser.add_option("-c", "--score", dest="score_train", default="data-train/es-en_score.train", help="Score train set")
optparser.add_option("-s", "--source", dest="source_train", default="data-train/es-en_source.train", help="Scourse train set")
optparser.add_option("-t", "--target", dest="target_train", default="data-train/es-en_target.train", help="Target train set")
optparser.add_option("-f", "--features", dest="feature_train", default="data-train/train_features", help="Feature train set")

# k = 9 for best result!
optparser.add_option("-k", "--k", dest="k", type="int", default=11, help="k nearest neighbors")
(opts, _) = optparser.parse_args()

score = [float(s.strip()) for s in open(opts.score_train).readlines()]
source = [s.strip() for s in open(opts.source_train).readlines()]
target = [s.strip().split() for s in open(opts.target_train).readlines()]
test_source = [s.strip() for s in open("data-test/es-en_source.test").readlines()]
test_target = [s.strip().split() for s in open("data-test/es-en_target.test").readlines()]
features = [s.strip().split() for s in open(opts.feature_train).readlines()]

cachedStopWordsEnglish = stopwords.words("english")
cachedStopWordsSpanish = stopwords.words("spanish")

english_vocab = set(w.lower() for w in nltk.corpus.words.words())

stemmer = PorterStemmer()

def get_untranslated_words(hyp):
  text_vocab = set(w.lower() for w in hyp if w.lower().isalpha())
  text_vocab_stemmed = set(stemmer.stem(word) for word in text_vocab)
  unusual = text_vocab_stemmed.difference(english_vocab)
  return len(unusual)

def stop_words_train():
    en = 0
    es = 0
    perc_content_es = [0]*len(source)
    for i,sentence in enumerate(source):
        length = len(sentence)
        es = 0
        for word in sentence:
            if word not in cachedStopWordsSpanish:
                es+=1
        perc_content_es[i] = es*100/float(length)
        # sys.stderr.write(str(perc_content_es[i])+"\n")
       # features[i].append(str(perc_content_es[i]))

    for i,sentence in enumerate(target):
        untrans = get_untranslated_words(sentence)
        features[i].append(str(untrans))
        length = len(sentence)
        en = 0
        for word in sentence:
            if word not in cachedStopWordsEnglish:
                en+=1
        perc_content_en = en*100/float(length)
        # features[i].append(str(perc_content_en))
        ratio = perc_content_es[i]/float(perc_content_en)
        features[i].append(str(ratio))


stop_words_train()


# Set the weights
weights = {}
for i, f in enumerate(features):
    weights[i] = 1

def get_distance(test, train):
    return abs(test - train)

def get_nearest_neighbors(sum_feats, k):
    dist_list = []
    for feat_sum, label in train_dict:
        dist = get_distance(sum_feats, feat_sum)
        dist_list.append((dist, label))

    sorted_list = sorted(dist_list, key = operator.itemgetter(0))
    return [tup[1] for tup in sorted_list[:k]]

def get_label(sum_feats):
    nearest_labels = get_nearest_neighbors(sum_feats, opts.k)
    mode = max(set(nearest_labels), key = nearest_labels.count)
    return mode

def feat_sum(feature_list):
    total = 0
    for w, f in enumerate(feature_list):
        total += float(f) * weights[w]
    return total

f = []
for feat_list in features:
    feats = []
    for feat in feat_list:
        feats.append(float(feat))
    f.append(feats)
# print f
X_train = f
Y_train = score
ols = linear_model.LinearRegression()
ols.fit(X_train, Y_train)

for i, w in enumerate(ols.coef_):
    weights[i] = w

# Read all sentences along with labels and features
train_dict = list()
for i, feat in enumerate(features):
    train_dict.append((feat_sum(feat), score[i]))

test_features = [s.strip().split() for s in open("data-test/test_features")]
def stop_words_test():
    en = 0
    es = 0
    perc_content_es = [0]*len(test_source)
    for i,sentence in enumerate(test_source):
        length = len(sentence)
        es = 0
        for word in sentence:
            if word not in cachedStopWordsSpanish:
                es+=1
        perc_content_es[i] = es*100/float(length)
        # test_features[i].append(str(perc_content_es[i]))

    for i,sentence in enumerate(test_target):
        untrans = get_untranslated_words(sentence)
        features[i].append(str(untrans))
        length = len(sentence)
        en = 0
        for word in sentence:
            if word not in cachedStopWordsEnglish:
                en+=1
        perc_content_en = en*100/float(length)
        # test_features[i].append(str(perc_content_en))
        ratio = perc_content_es[i]/float(perc_content_en)
        features[i].append(str(ratio))


test_scores = {}
for i, feats in enumerate(test_features):
    sum_feats = feat_sum(feats)
    test_scores[i] = int(get_label(sum_feats))

for score in test_scores.values():
    sys.stdout.write("%s\n" % score)